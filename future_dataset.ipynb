{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Updating counts with new transit stations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\domin\\AppData\\Local\\Temp\\ipykernel_18848\\186224853.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_stations['Closest_Junction_ID'] = new_stations.apply(calculate_nearest_junction, axis=1, junctions_df=junctions_2023)\n",
      "C:\\Users\\domin\\AppData\\Local\\Temp\\ipykernel_18848\\186224853.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  junctions_with_transit_counts['Updated_Transit_Count'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated dataset with transit counts based on nearest junctions has been saved as 'future_dataset_transit_counts.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import distance\n",
    "\n",
    "# Load final dataset containing counts and junctions duplicated for every year\n",
    "final_dataset = pd.read_excel('final_dataset_count.xlsx')\n",
    "\n",
    "# Filter junctions for the year 2023\n",
    "junctions_2023 = final_dataset[final_dataset['year'] == 2023]\n",
    "\n",
    "# Load new transit stations dataset\n",
    "transit_stations = pd.read_excel('transit_stations.xlsx')\n",
    "\n",
    "# Filter for the new stations\n",
    "new_stations = transit_stations[transit_stations['year'] == 2027]\n",
    "\n",
    "# Function to calculate distance using geopy.distance\n",
    "def calculate_nearest_junction(station_row, junctions_df):\n",
    "    station_coords = (station_row['latitude'], station_row['longitude'])\n",
    "    distances = junctions_df.apply(lambda row: distance(station_coords, (row['latitude'], row['longitude'])).km, axis=1)\n",
    "    closest_junction_index = distances.idxmin()\n",
    "    return junctions_df.loc[closest_junction_index, 'id']\n",
    "\n",
    "# Find the nearest junction for each transit station\n",
    "new_stations['Closest_Junction_ID'] = new_stations.apply(calculate_nearest_junction, axis=1, junctions_df=junctions_2023)\n",
    "\n",
    "# Calculate updated transit counts based on closest junctions\n",
    "transit_counts_by_junction = new_stations['Closest_Junction_ID'].value_counts().reset_index()\n",
    "transit_counts_by_junction.columns = ['id', 'Updated_Transit_Count']\n",
    "\n",
    "# Merge with junctions_2023 to get all junctions with their updated transit counts\n",
    "junctions_with_transit_counts = junctions_2023.merge(transit_counts_by_junction, on='id', how='left')\n",
    "junctions_with_transit_counts['Updated_Transit_Count'].fillna(0, inplace=True)\n",
    "junctions_with_transit_counts['Updated_Transit_Count'] = junctions_with_transit_counts['Updated_Transit_Count'].astype(int)\n",
    "\n",
    "# Optionally, you may want to combine previous transit counts with the updated counts\n",
    "# Assuming 'transit_Count' is the column in junctions_2023 containing original counts\n",
    "junctions_with_transit_counts['Combined_Transit_Count'] = junctions_with_transit_counts['transit_count'] + junctions_with_transit_counts['Updated_Transit_Count']\n",
    "\n",
    "# Delete 'transit_count' and 'Updated_Transit_Count' columns\n",
    "junctions_with_transit_counts.drop(columns=['transit_count', 'Updated_Transit_Count'], inplace=True)\n",
    "\n",
    "# Rename 'Combined_Transit_Count' to 'transit_count'\n",
    "junctions_with_transit_counts.rename(columns={'Combined_Transit_Count': 'transit_count'}, inplace=True)\n",
    "\n",
    "# Change all values in year column to 2024\n",
    "junctions_with_transit_counts.loc[:, 'year'] = 2024\n",
    "\n",
    "# Save the updated dataset with junctions and their transit counts to a new file\n",
    "junctions_with_transit_counts.to_excel('future_dataset_transit_counts.xlsx', index=False)\n",
    "\n",
    "print(\"The updated dataset with transit counts based on nearest junctions has been saved as 'future_dataset_transit_counts.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store Counts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cleaning the business-licences*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned business licences data has been saved as 'cleaned_business_2024.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load business licences data\n",
    "business_licences = pd.read_excel('business-licences.xlsx')\n",
    "\n",
    "# Load cleaned and transformed business licences data\n",
    "cleaned_transformed_business_licences = pd.read_excel('cleaned_transformed_business_licences.xlsx')\n",
    "\n",
    "# Step 1: Filter out business types not present in cleaned_transformed_business_licences\n",
    "valid_business_types = cleaned_transformed_business_licences['businesstype'].unique()\n",
    "filtered_business_licences = business_licences[business_licences['BusinessType'].isin(valid_business_types)]\n",
    "\n",
    "# Step 2: Filter by city 'Vancouver'\n",
    "filtered_business_licences = filtered_business_licences[filtered_business_licences['City'] == 'Vancouver']\n",
    "\n",
    "# Step 3: Select rows that do not have an empty value for 'geo_point_2d'\n",
    "filtered_business_licences = filtered_business_licences.dropna(subset=['geo_point_2d'])\n",
    "\n",
    "# Step 4: Drop rows where 'IssuedDate' or 'ExpiredDate' is NaN\n",
    "filtered_business_licences = filtered_business_licences.dropna(subset=['IssuedDate', 'ExpiredDate'])\n",
    "\n",
    "# Step 5: Extract rows where 'IssuedDate' starts with '2024'\n",
    "filtered_business_licences = filtered_business_licences[filtered_business_licences['IssuedDate'].str.startswith('2024')]\n",
    "\n",
    "# Step 6: Remove rows where 'ExpiredDate' is not '2024-12-31'\n",
    "filtered_business_licences = filtered_business_licences[filtered_business_licences['ExpiredDate'] == '2024-12-31']\n",
    "\n",
    "# Save the cleaned data to a new Excel file\n",
    "filtered_business_licences.to_excel('cleaned_business_2024.xlsx', index=False)\n",
    "\n",
    "print(\"The cleaned business licences data has been saved as 'cleaned_business_2024.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Updating store counts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\domin\\AppData\\Local\\Temp\\ipykernel_18848\\1727660218.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  junctions_with_store_counts['Updated_Store_Count'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated dataset with store counts based on nearest junctions has been saved as 'future_dataset_count.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import distance\n",
    "\n",
    "# Load future dataset containing junctions and their transit counts\n",
    "junctions_with_transit_counts = pd.read_excel('future_dataset_transit_counts.xlsx')\n",
    "\n",
    "# Load cleaned business licenses data\n",
    "cleaned_business_licenses = pd.read_excel('cleaned_business_2024.xlsx')\n",
    "\n",
    "# Extract latitude and longitude from 'geo_point_2d'\n",
    "cleaned_business_licenses[['latitude', 'longitude']] = cleaned_business_licenses['geo_point_2d'].str.split(',', expand=True).astype(float)\n",
    "\n",
    "# Function to calculate the nearest junction for a given store\n",
    "def calculate_nearest_junction(store_row, junctions_df):\n",
    "    store_coords = (store_row['latitude'], store_row['longitude'])\n",
    "    distances = junctions_df.apply(lambda row: distance(store_coords, (row['latitude'], row['longitude'])).km, axis=1)\n",
    "    closest_junction_index = distances.idxmin()\n",
    "    return junctions_df.loc[closest_junction_index, 'id']\n",
    "\n",
    "# Find the nearest junction for each store\n",
    "cleaned_business_licenses['Closest_Junction_ID'] = cleaned_business_licenses.apply(calculate_nearest_junction, axis=1, junctions_df=junctions_with_transit_counts)\n",
    "\n",
    "# Calculate updated store counts based on closest junctions\n",
    "store_counts_by_junction = cleaned_business_licenses['Closest_Junction_ID'].value_counts().reset_index()\n",
    "store_counts_by_junction.columns = ['id', 'Updated_Store_Count']\n",
    "\n",
    "# Merge with junctions_with_transit_counts to get all junctions with their updated store counts\n",
    "junctions_with_store_counts = junctions_with_transit_counts.merge(store_counts_by_junction, on='id', how='left')\n",
    "junctions_with_store_counts['Updated_Store_Count'].fillna(0, inplace=True)\n",
    "junctions_with_store_counts['Updated_Store_Count'] = junctions_with_store_counts['Updated_Store_Count'].astype(int)\n",
    "\n",
    "# Combine previous store counts with the updated counts\n",
    "# Assuming 'store_count' is the column in junctions_with_transit_counts containing original counts\n",
    "junctions_with_store_counts['Combined_Store_Count'] = junctions_with_store_counts['store_count'] + junctions_with_store_counts['Updated_Store_Count']\n",
    "\n",
    "# Delete 'store_count' and 'Updated_Store_Count' columns\n",
    "junctions_with_store_counts.drop(columns=['store_count', 'Updated_Store_Count'], inplace=True)\n",
    "\n",
    "# Rename 'Combined_Store_Count' to 'store_count'\n",
    "junctions_with_store_counts.rename(columns={'Combined_Store_Count': 'store_count'}, inplace=True)\n",
    "\n",
    "# Save the updated dataset with junctions and their store counts to a new file\n",
    "junctions_with_store_counts.to_excel('future_dataset_count.xlsx', index=False)\n",
    "\n",
    "print(\"The updated dataset with store counts based on nearest junctions has been saved as 'future_dataset_count.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating Reaches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated 6179/6179           \n",
      "Normalizing\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Calculate the reaches of the time invariant features\n",
    "\n",
    "import sys\n",
    "sys.path.append('../') # This should probably be changed to a more sofisticated system at some point. i.e. install the package\n",
    "\n",
    "import math\n",
    "\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "from ast import literal_eval\n",
    "from data_wrangler.dataset import Dataset\n",
    "\n",
    "JUNCTION_FILE = 'future_dataset_count.csv'\n",
    "\n",
    "CRIME_SIGMA = 132\n",
    "STANDARD_DEVIATION = 400\n",
    "\n",
    "junctions = Dataset.load_file(JUNCTION_FILE)\n",
    "junctions.convert_properties({\n",
    "    'id': int,\n",
    "    'crime_count': int,\n",
    "    'store_count': int,\n",
    "    'police_count': int,\n",
    "    'transit_count': int,\n",
    "    'graffiti_count': int,\n",
    "    'homeless_shelter_count': int,\n",
    "    'traffic_signal_count': int,\n",
    "    'street_lighting_poles_count': int,\n",
    "    'schools_count': int,\n",
    "    'neighbors': lambda v : literal_eval(v) if v else []\n",
    "})\n",
    "\n",
    "def normal_dst(distance, standard_deviation):\n",
    "    scale = 1 / (2 * math.pi * (standard_deviation ** 2))\n",
    "    power = distance ** 2 / (2 * standard_deviation ** 2)\n",
    "    distribution = math.exp(-power)\n",
    "    return scale * distribution\n",
    "\n",
    "def reach_dst(distance, scale):\n",
    "    \"\"\" Calculate a modified version of Borgatti's reach formula\n",
    "    \n",
    "    TODO: Check that convergence is important and that if is whether we actually need to cube the denominator\n",
    "    The range formula is:  sumweight * 1 / (dst_scale * dst + 1) ^ 2\n",
    "    We have +1 because we want distance of zero to be constant with respect to dst_scale\n",
    "    We cube the denominator because this causes it to converge\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1 / ((distance / scale + 1) ** 3)\n",
    "\n",
    "def calculate_reach(junction, properties, dst_func, limit=float('inf')):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        junction (Row): The junction to calculate the reach for\n",
    "        prop (str): The property to use for junction weights\n",
    "        dst_scale (float): The value to scale distance by. Should be in the range (0, 1]. Likely close to zero.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated reach.\n",
    "    \"\"\"\n",
    "    reaches = { key: 0 for key in properties}\n",
    "    visited = set()\n",
    "    queue = []\n",
    "    heappush(queue, (0, junction['id']))\n",
    "    while queue:\n",
    "        dst, next_jun = heappop(queue)\n",
    "        if next_jun in visited: continue\n",
    "        visited.add(next_jun)\n",
    "        if dst > limit: continue\n",
    "        \n",
    "        # The range formula is: weight * 1 / (dst_scale * dst + 1) ^ 2\n",
    "        # We have +1 because we want distance of zero to be constant with respect to dst_scale\n",
    "        # We square the denominator because this causes it to converge\n",
    "        \n",
    "        # Update the range values\n",
    "        crime_dst = normal_dst(dst, CRIME_SIGMA)\n",
    "        scaled_dst = dst_func(dst)\n",
    "        for key in properties:\n",
    "            if key == 'crime_reach':\n",
    "                reaches[key] += junctions[next_jun][properties[key]] * crime_dst\n",
    "            else:\n",
    "                reaches[key] += junctions[next_jun][properties[key]] * scaled_dst\n",
    "              \n",
    "        for neighbor, delta, s_id in junctions[next_jun]['neighbors']:\n",
    "            if neighbor in visited: continue\n",
    "            neighbor_dst = dst + delta\n",
    "            heappush(queue, (neighbor_dst, neighbor))\n",
    "    return reaches\n",
    "\n",
    "def calculate_reaches(junctions, properties, dst_func, limit=float('inf')):\n",
    "    highest = { key: 0 for key in properties}\n",
    "    \n",
    "    for i, junction in enumerate(junctions):\n",
    "        reaches = calculate_reach(junction, properties, dst_func, limit)\n",
    "        for key in reaches:\n",
    "            junction[key] = reaches[key]\n",
    "            highest[key] = max(highest[key], reaches[key])\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'\\rCalculated {i+1}/{len(junctions)}           ', end='')\n",
    "    print(f'\\rCalculated {len(junctions)}/{len(junctions)}        ')\n",
    "    print(\"Normalizing\")\n",
    "    for junction in junctions:\n",
    "        for key in properties:\n",
    "            junction[key] /= highest[key]\n",
    "    print(\"Done\")\n",
    "    \n",
    "calculate_reaches(\n",
    "    junctions, \n",
    "    {\n",
    "        # 'crime_reach': 'crime_count',\n",
    "        'store_reach': 'store_count',\n",
    "        # 'police_reach': 'police_count',\n",
    "        'transit_reach': 'transit_count',\n",
    "        # 'graffiti_reach': 'graffiti_count',\n",
    "        # 'homeless_shelter_reach': 'homeless_shelter_count',\n",
    "        # 'traffic_signal_reach': 'traffic_signal_count',\n",
    "        # 'street_lighting_poles_reach': 'street_lighting_poles_count',\n",
    "        # 'schools_reach': 'schools_count'\n",
    "    }, \n",
    "    lambda dst: normal_dst(dst, STANDARD_DEVIATION),\n",
    "    limit=1000\n",
    ")\n",
    "junctions.write_to_file('future_dataset_reach.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging Future Dataset with new dataset that has more features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to extract: 'neighborhood_id', 'Census Population', 'English (as mother tongue)', 'Unemployment Rate', 'Public Transport', 'Median Household Income', 'Population in Low-Income Households'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "merf_df = pd.read_excel('data\\\\merged_final_with_updated_features.xlsx')\n",
    "future_df = pd.read_excel('data\\\\future_dataset.xlsx')\n",
    "\n",
    "# Filter for the year 2023\n",
    "merf_2023_df = merf_df[merf_df['Year'] == 2023]\n",
    "\n",
    "# Select the required columns\n",
    "columns_to_extract = ['id', 'neighborhood_id', 'Census Population', 'English (as mother tongue)', \n",
    "                      'Unemployment Rate', 'Public Transport', 'Median Household Income', \n",
    "                      'Population in Low-Income Households']\n",
    "merf_2023_selected_df = merf_2023_df[columns_to_extract]\n",
    "\n",
    "# Merge datasets on the 'id' column\n",
    "merged_df = pd.merge(future_df, merf_2023_selected_df, on='id', how='left')\n",
    "\n",
    "# Save the merged dataframe to a new Excel file\n",
    "merged_df.to_excel('data\\\\future_dataset_merged.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double checking the transit reaches are changing in a way that makes sense**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id      type  year  street_count   longitude   latitude  elevation  \\\n",
      "0   1  Junction  2024             2 -123.224758  49.274760         65   \n",
      "1   4  Junction  2024             4 -123.220892  49.271168         90   \n",
      "2   5  Junction  2024             3 -123.220567  49.273832         71   \n",
      "3   6  Junction  2024             3 -123.219613  49.268957         93   \n",
      "4   7  Junction  2024             2 -123.219572  49.267203         97   \n",
      "\n",
      "   average_segments_length  betweenness_centrality  \\\n",
      "0               396.227813            0.000000e+00   \n",
      "1               480.393636            2.100000e-07   \n",
      "2               522.393980            3.214230e-04   \n",
      "3               265.464150            3.203230e-04   \n",
      "4               261.840633            4.610000e-06   \n",
      "\n",
      "   scaled_betweenness_centrality  ... street_lighting_poles_reach  \\\n",
      "0                       0.000000  ...                    0.024386   \n",
      "1                       0.020964  ...                    0.067050   \n",
      "2                      32.142337  ...                    0.055684   \n",
      "3                      32.032279  ...                    0.085402   \n",
      "4                       0.461198  ...                    0.084759   \n",
      "\n",
      "   schools_reach  neighborhood_id  Census Population  \\\n",
      "0            0.0               22            13117.5   \n",
      "1            0.0               22            13117.5   \n",
      "2            0.0               22            13117.5   \n",
      "3            0.0               22            13117.5   \n",
      "4            0.0               22            13117.5   \n",
      "\n",
      "   English (as mother tongue)  Unemployment Rate  Public Transport  \\\n",
      "0                        58.0               7.31             33.69   \n",
      "1                        58.0               7.31             33.69   \n",
      "2                        58.0               7.31             33.69   \n",
      "3                        58.0               7.31             33.69   \n",
      "4                        58.0               7.31             33.69   \n",
      "\n",
      "   Median Household Income  Population in Low-Income Households  \\\n",
      "0                  80873.5                                20.29   \n",
      "1                  80873.5                                20.29   \n",
      "2                  80873.5                                20.29   \n",
      "3                  80873.5                                20.29   \n",
      "4                  80873.5                                20.29   \n",
      "\n",
      "   transit_reach_difference  \n",
      "0                       0.0  \n",
      "1                       0.0  \n",
      "2                       0.0  \n",
      "3                       0.0  \n",
      "4                       0.0  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "merged_final_file = \"data\\\\merged_final_with_updated_features.xlsx\"\n",
    "future_dataset_file = \"data\\\\future_dataset_merged.xlsx\"\n",
    "\n",
    "merged_final_df = pd.read_excel(merged_final_file)\n",
    "future_dataset_df = pd.read_excel(future_dataset_file)\n",
    "\n",
    "# Filter merged_final_df for Year == 2023\n",
    "merged_final_2023 = merged_final_df[merged_final_df['Year'] == 2023]\n",
    "\n",
    "# Calculate differences for each junction\n",
    "for index, row in merged_final_2023.iterrows():\n",
    "    junction = row['id']\n",
    "    transit_reach_2023 = row['transit_reach']\n",
    "    \n",
    "    # Find corresponding row in future_dataset_df\n",
    "    future_row = future_dataset_df[future_dataset_df['id'] == junction]\n",
    "    \n",
    "    if not future_row.empty:\n",
    "        future_dataset_df.loc[future_dataset_df['id'] == junction, 'transit_reach_difference'] = future_row['transit_reach'].values[0] - transit_reach_2023\n",
    "\n",
    "# Save the updated DataFrame if needed\n",
    "output_file = \"future_dataset_merged_with_difference.xlsx\"\n",
    "future_dataset_df.to_excel(output_file, index=False)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(future_dataset_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
