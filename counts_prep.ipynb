{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**betweenness centrality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating betweenness_centrality\n"
     ]
    }
   ],
   "source": [
    "# graph of Vancouver\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Load datasets\n",
    "segments_df = pd.read_csv('streetsegments.csv') # has the connections\n",
    "\n",
    "weights_df = pd.read_csv('segments.csv') # has the weights\n",
    "\n",
    "# Initialize graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Function to add edges based on non-zero junction IDs\n",
    "def add_edge_based_on_junctions(graph, row, length):\n",
    "    junctions = []\n",
    "    if row['pseudoJunctionID1'] != 0:\n",
    "        junctions.append(row['pseudoJunctionID1'])\n",
    "    if row['pseudoJunctionID2'] != 0:\n",
    "        junctions.append(row['pseudoJunctionID2'])\n",
    "    if row['adjustJunctionID1'] != 0:\n",
    "        junctions.append(row['adjustJunctionID1'])\n",
    "    if row['adjustJunctionID2'] != 0:\n",
    "        junctions.append(row['adjustJunctionID2'])\n",
    "    \n",
    "    if len(junctions) == 2:\n",
    "        segment_id = row['StreetID']\n",
    "        length_metres = length.get(segment_id, 0) # length of 0 default\n",
    "        graph.add_edge(junctions[0], junctions[1], id=segment_id, weight=length_metres)\n",
    "\n",
    "# Create a dictionary from weights_df for easy lookup\n",
    "length_dict = dict(zip(weights_df['id'], weights_df['length_metres']))\n",
    "\n",
    "# Add edges based on junction IDs\n",
    "for index, row in segments_df.iterrows():\n",
    "    add_edge_based_on_junctions(G, row, length_dict)\n",
    "\n",
    "# Calculate betweenness centrality\n",
    "print('calculating betweenness_centrality')\n",
    "betweenness_centrality = nx.betweenness_centrality(G, weight='weight', normalized=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to betweenness_centrality_results.xlsx\n",
      "Number of nodes: 6179\n",
      "Number of edges: 10503\n"
     ]
    }
   ],
   "source": [
    "# WRITING BETWEENNESS CENTRALITY TO NEW SHEET\n",
    "# Prepare data for Excel\n",
    "data = [{'id': node, 'betweenness_centrality': betweenness_centrality} for node, betweenness_centrality in betweenness_centrality.items()]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Write to Excel\n",
    "excel_file = 'betweenness_centrality_results.xlsx'\n",
    "df.to_excel(excel_file, index=False)\n",
    "\n",
    "print(f\"Data successfully written to {excel_file}\")\n",
    "\n",
    "# Optional: Check the resulting graph\n",
    "print(\"Number of nodes:\", G.number_of_nodes())\n",
    "print(\"Number of edges:\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset with betweenness centrality saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# MERGING BETWEENNESS CENTRALITY WITH MAIN SHEET\n",
    "import pandas as pd\n",
    "\n",
    "# Load betweenness centrality data\n",
    "betweenness_df = pd.read_excel('betweenness_centrality.xlsx')\n",
    "\n",
    "# Load the target dataset where we want to add the betweenness centrality column\n",
    "final_dataset_reach = pd.read_excel('final_dataset_reach.xlsx')\n",
    "\n",
    "# Merge betweenness centrality data with final_dataset_reach based on 'Junction ID'\n",
    "merged_dataset = pd.merge(final_dataset_reach, betweenness_df, on='id', how='left')\n",
    "\n",
    "# Write the updated dataset to Excel with the added column\n",
    "merged_dataset.to_excel('final_dataset_with_centrality.xlsx', index=False)\n",
    "\n",
    "print(\"Updated dataset with betweenness centrality saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graffiti counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDING GRAFFITI COUNTS TO MAIN SHEET\n",
    "# will be used to calculate reach\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "graffiti_df = pd.read_csv('graffiti.csv')\n",
    "target_df = pd.read_excel('final_dataset_with_centrality.xlsx')\n",
    "\n",
    "# Aggregate graffiti counts by junctionID\n",
    "graffiti_aggregated = graffiti_df.groupby('junction_id')['count'].sum().reset_index()\n",
    "\n",
    "# Rename the 'count' column to 'graffiti_count' for clarity\n",
    "graffiti_aggregated.rename(columns={'count': 'graffiti_count'}, inplace=True)\n",
    "\n",
    "# Merge the aggregated graffiti counts with the target dataset\n",
    "merged_df = target_df.merge(graffiti_aggregated, how='left', left_on='id', right_on='junction_id')\n",
    "\n",
    "# Drop the 'junction_id' column as it is no longer needed in merged_df\n",
    "merged_df.drop(columns=['junction_id'], inplace=True)\n",
    "\n",
    "# Fill NaN values in the 'graffiti_count' column with 0 (if there are any junctions with no graffiti counts)\n",
    "merged_df['graffiti_count'] = merged_df['graffiti_count'].fillna(0)\n",
    "\n",
    "# Save the result to a new CSV file (or overwrite the existing one if needed)\n",
    "merged_df.to_excel('final_dataset_graffiti_count.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homeless Shelter Counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated dataset with homeless shelter counts has been saved as 'final_dataset_shelter_counts.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "# HOMELESS SHELTER COUNTS\n",
    "\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Load data from Excel files\n",
    "shelters_df = pd.read_excel('homeless-shelter-locations.xlsx')\n",
    "junctions_df = pd.read_excel('junctions.xlsx')\n",
    "\n",
    "# Extract latitude and longitude for homeless shelters\n",
    "shelters_df[['latitude', 'longitude']] = shelters_df['geo_point_2d'].str.split(',', expand=True).astype(float)\n",
    "\n",
    "# Function to find the nearest junction\n",
    "def find_nearest_junction(shelter_lat, shelter_lon, junctions_df):\n",
    "    min_distance = float('inf')\n",
    "    nearest_junction_id = None\n",
    "    \n",
    "    for _, junction in junctions_df.iterrows():\n",
    "        junction_lat = junction['latitude']\n",
    "        junction_lon = junction['longitude']\n",
    "        distance = geodesic((shelter_lat, shelter_lon), (junction_lat, junction_lon)).meters\n",
    "        \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest_junction_id = junction['id']\n",
    "    \n",
    "    return nearest_junction_id\n",
    "\n",
    "# Apply the function to each shelter to find the nearest junction\n",
    "shelters_df['nearest_junction_id'] = shelters_df.apply(\n",
    "    lambda row: find_nearest_junction(row['latitude'], row['longitude'], junctions_df), axis=1)\n",
    "\n",
    "# Count the number of shelters assigned to each junction\n",
    "junction_shelter_count = shelters_df['nearest_junction_id'].value_counts().reset_index()\n",
    "junction_shelter_count.columns = ['id', 'homeless_shelter_count']\n",
    "\n",
    "# Load the final dataset\n",
    "final_dataset = pd.read_excel('final_dataset_graffiti_count.xlsx')\n",
    "\n",
    "# Merge the homeless shelter counts with the final_dataset\n",
    "final_dataset_with_counts = final_dataset.merge(junction_shelter_count, on='id', how='left').fillna(0)\n",
    "final_dataset_with_counts['homeless_shelter_count'] = final_dataset_with_counts['homeless_shelter_count'].astype(int)\n",
    "\n",
    "# Save the updated dataset to a new file\n",
    "final_dataset_with_counts.to_excel('final_dataset_shelter_counts.xlsx', index=False)\n",
    "\n",
    "print(\"The updated dataset with homeless shelter counts has been saved as 'final_dataset_shelter_counts.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAFFIC SIGNAL COUNTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated dataset with traffic signal counts has been saved as 'final_dataset_traffic_signal_counts.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "# TRAFFIC SIGNAL COUNTS\n",
    "\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Load data from Excel files\n",
    "signals_df = pd.read_excel('traffic-signals.xlsx')\n",
    "junctions_df = pd.read_excel('junctions.xlsx')\n",
    "\n",
    "# Extract latitude and longitude for traffic signals\n",
    "signals_df[['latitude', 'longitude']] = signals_df['geo_point_2d'].str.split(',', expand=True).astype(float)\n",
    "\n",
    "# Function to find the nearest junction for a traffic signal\n",
    "def find_nearest_junction(signal_lat, signal_lon, junctions_df):\n",
    "    min_distance = float('inf')\n",
    "    nearest_junction_id = None\n",
    "    \n",
    "    for _, junction in junctions_df.iterrows():\n",
    "        junction_lat = junction['latitude']\n",
    "        junction_lon = junction['longitude']\n",
    "        distance = geodesic((signal_lat, signal_lon), (junction_lat, junction_lon)).meters\n",
    "        \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest_junction_id = junction['id']\n",
    "    \n",
    "    return nearest_junction_id\n",
    "\n",
    "# Apply the function to each traffic signal to find the nearest junction\n",
    "signals_df['nearest_junction_id'] = signals_df.apply(\n",
    "    lambda row: find_nearest_junction(row['latitude'], row['longitude'], junctions_df), axis=1)\n",
    "\n",
    "# Count the number of signals assigned to each junction\n",
    "junction_signal_count = signals_df['nearest_junction_id'].value_counts().reset_index()\n",
    "junction_signal_count.columns = ['id', 'traffic_signal_count']\n",
    "\n",
    "# Load the final dataset where you want to merge the traffic signal counts\n",
    "final_dataset = pd.read_excel('final_dataset_shelter_counts.xlsx')\n",
    "\n",
    "# Merge the traffic signal counts with the final_dataset\n",
    "final_dataset_with_counts = final_dataset.merge(junction_signal_count, on='id', how='left').fillna(0)\n",
    "final_dataset_with_counts['traffic_signal_count'] = final_dataset_with_counts['traffic_signal_count'].astype(int)\n",
    "\n",
    "# Save the updated dataset to a new file\n",
    "final_dataset_with_counts.to_excel('final_dataset_traffic_signal_counts.xlsx', index=False)\n",
    "\n",
    "print(\"The updated dataset with traffic signal counts has been saved as 'final_dataset_traffic_signal_counts.xlsx'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STREET LIGHT COUNTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated dataset with traffic signal counts has been saved as 'final_dataset_lights_counts.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "# STREET LIGHT COUNTS\n",
    "\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Load data from Excel files\n",
    "lights_df = pd.read_excel('street-lighting-poles.xlsx')\n",
    "junctions_df = pd.read_excel('junctions.xlsx')\n",
    "\n",
    "# Extract latitude and longitude for traffic signals\n",
    "lights_df[['latitude', 'longitude']] = lights_df['geo_point_2d'].str.split(',', expand=True).astype(float)\n",
    "\n",
    "# Function to find the nearest junction for a traffic signal\n",
    "def find_nearest_junction(light_lat, light_lon, junctions_df):\n",
    "    min_distance = float('inf')\n",
    "    nearest_junction_id = None\n",
    "    \n",
    "    for _, junction in junctions_df.iterrows():\n",
    "        junction_lat = junction['latitude']\n",
    "        junction_lon = junction['longitude']\n",
    "        distance = geodesic((light_lat, light_lon), (junction_lat, junction_lon)).meters\n",
    "        \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest_junction_id = junction['id']\n",
    "    \n",
    "    return nearest_junction_id\n",
    "\n",
    "# Apply the function to each traffic signal to find the nearest junction\n",
    "lights_df['nearest_junction_id'] = lights_df.apply(\n",
    "    lambda row: find_nearest_junction(row['latitude'], row['longitude'], junctions_df), axis=1)\n",
    "\n",
    "# Count the number of signals assigned to each junction\n",
    "junction_lights_count = lights_df['nearest_junction_id'].value_counts().reset_index()\n",
    "junction_lights_count.columns = ['id', 'street_lighting_poles_count']\n",
    "\n",
    "# Load the final dataset where you want to0 merge the traffic signal counts\n",
    "final_dataset = pd.read_excel('final_dataset_traffic_signal_counts.xlsx')\n",
    "\n",
    "# Merge the traffic signal counts with the final_dataset\n",
    "final_dataset_with_counts = final_dataset.merge(junction_lights_count, on='id', how='left').fillna(0)\n",
    "final_dataset_with_counts['street_lighting_poles_count'] = final_dataset_with_counts['street_lighting_poles_count'].astype(int)\n",
    "\n",
    "# Save the updated dataset to a new file\n",
    "final_dataset_with_counts.to_excel('final_dataset_lights_counts.xlsx', index=False)\n",
    "\n",
    "print(\"The updated dataset with traffic signal counts has been saved as 'final_dataset_lights_counts.xlsx'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
